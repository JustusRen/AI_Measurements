{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 20:21:41.957267: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-20 20:21:41.957292: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from common import *\n",
    "import tarfile\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from keras.backend import clear_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 100000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Previous reviewer Claudio Carvalho gave a much...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CONTAINS \"SPOILER\" INFORMATION. Watch this dir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is my first Deepa Mehta film. I saw the f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This was a great film in every sense of the wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A stunningly well-made film, with exceptional ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Previous reviewer Claudio Carvalho gave a much...      1\n",
       "1  CONTAINS \"SPOILER\" INFORMATION. Watch this dir...      1\n",
       "2  This is my first Deepa Mehta film. I saw the f...      1\n",
       "3  This was a great film in every sense of the wo...      1\n",
       "4  A stunningly well-made film, with exceptional ...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists('aclImdb'):\n",
    "    if not os.path.exists('aclImdb_v1.tar.gz'):\n",
    "        files = download_data()\n",
    "    else:\n",
    "        files = ['aclImdb_v1.tar.gz']\n",
    "    \n",
    "    tarfile.open(files[0]).extractall()\n",
    "\n",
    "paths = glob.glob('aclImdb/**/pos/*.txt', recursive=True)\n",
    "pos_frame = get_data(paths, 1)\n",
    "paths = glob.glob('aclImdb/**/neg/*.txt', recursive=True)\n",
    "neg_frame = get_data(paths, 0)\n",
    "\n",
    "df = pd.concat([pos_frame, neg_frame])\n",
    "print(f'Size of dataset: {df.size}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>I just don't get these reviews! I can't help t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10067</th>\n",
       "      <td>Foolish hikers go camping in the Utah mountain...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9590</th>\n",
       "      <td>This movie is pretty predictable nuff said.......</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16668</th>\n",
       "      <td>A convict serving time comes forward to give t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12196</th>\n",
       "      <td>This is a love story set against the back drop...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "1247   I just don't get these reviews! I can't help t...      0\n",
       "10067  Foolish hikers go camping in the Utah mountain...      0\n",
       "9590   This movie is pretty predictable nuff said.......      0\n",
       "16668  A convict serving time comes forward to give t...      1\n",
       "12196  This is a love story set against the back drop...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(n=NUM_SAMPLES, random_state=RANDOM_SEED)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2500, 51703)\n",
      "Test shape: (2500, 51703)\n"
     ]
    }
   ],
   "source": [
    "countVectorizer = CountVectorizer()\n",
    "embeddings = preprocess_data(df['text'], countVectorizer)\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts= train_test_split(embeddings, df['label'], test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "X_tr = X_tr.toarray()\n",
    "X_ts = X_ts.toarray()\n",
    "\n",
    "feature_size = X_tr.shape[1]\n",
    "\n",
    "print(f'Train shape: {X_tr.shape}')\n",
    "print(f'Test shape: {X_ts.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 10)                517040    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 517,051\n",
      "Trainable params: 517,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 20:21:49.207896: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-20 20:21:49.213369: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-20 20:21:49.213439: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fedora): /proc/driver/nvidia/version does not exist\n",
      "2022-04-20 20:21:49.223866: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#first basic ann\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "input_dim = X_tr.shape[1]  # Number of features\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 20:21:49.391510: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1034060000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "247/250 [============================>.] - ETA: 0s - loss: 0.6340 - accuracy: 0.6810"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 20:21:53.172309: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1034060000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 6s 21ms/step - loss: 0.6331 - accuracy: 0.6828 - val_loss: 0.5737 - val_accuracy: 0.8372\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.3862 - accuracy: 0.8916 - val_loss: 0.4065 - val_accuracy: 0.8304\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.1261 - accuracy: 0.9768 - val_loss: 0.3879 - val_accuracy: 0.8432\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.0408 - accuracy: 0.9964 - val_loss: 0.3514 - val_accuracy: 0.8640\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.0172 - accuracy: 0.9996 - val_loss: 0.3831 - val_accuracy: 0.8584\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.4026 - val_accuracy: 0.8588\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.4025 - val_accuracy: 0.8608\n",
      "Epoch 8/30\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.4209 - val_accuracy: 0.8608\n",
      "Epoch 9/30\n",
      "250/250 [==============================] - 4s 14ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4339 - val_accuracy: 0.8604\n",
      "Epoch 10/30\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4426 - val_accuracy: 0.8604\n",
      "Epoch 11/30\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4537 - val_accuracy: 0.8592\n",
      "Epoch 12/30\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4649 - val_accuracy: 0.8600\n",
      "Epoch 13/30\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 8.2172e-04 - accuracy: 1.0000 - val_loss: 0.4766 - val_accuracy: 0.8588\n",
      "Epoch 14/30\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 6.5564e-04 - accuracy: 1.0000 - val_loss: 0.4865 - val_accuracy: 0.8584\n",
      "Epoch 15/30\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 5.2597e-04 - accuracy: 1.0000 - val_loss: 0.4965 - val_accuracy: 0.8576\n",
      "Epoch 16/30\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 4.2934e-04 - accuracy: 1.0000 - val_loss: 0.5055 - val_accuracy: 0.8580\n",
      "Epoch 17/30\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 3.5277e-04 - accuracy: 1.0000 - val_loss: 0.5153 - val_accuracy: 0.8572\n",
      "Epoch 18/30\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 2.9167e-04 - accuracy: 1.0000 - val_loss: 0.5243 - val_accuracy: 0.8568\n",
      "Epoch 19/30\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 2.4288e-04 - accuracy: 1.0000 - val_loss: 0.5342 - val_accuracy: 0.8564\n",
      "Epoch 20/30\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 2.0309e-04 - accuracy: 1.0000 - val_loss: 0.5428 - val_accuracy: 0.8552\n",
      "Epoch 21/30\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 1.7108e-04 - accuracy: 1.0000 - val_loss: 0.5511 - val_accuracy: 0.8560\n",
      "Epoch 22/30\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 1.4369e-04 - accuracy: 1.0000 - val_loss: 0.5592 - val_accuracy: 0.8552\n",
      "Epoch 23/30\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 1.2190e-04 - accuracy: 1.0000 - val_loss: 0.5691 - val_accuracy: 0.8556\n",
      "Epoch 24/30\n",
      "250/250 [==============================] - 3s 10ms/step - loss: 1.0326e-04 - accuracy: 1.0000 - val_loss: 0.5765 - val_accuracy: 0.8552\n",
      "Epoch 25/30\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 8.7769e-05 - accuracy: 1.0000 - val_loss: 0.5854 - val_accuracy: 0.8540\n",
      "Epoch 26/30\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 7.4806e-05 - accuracy: 1.0000 - val_loss: 0.5941 - val_accuracy: 0.8536\n",
      "Epoch 27/30\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 6.3690e-05 - accuracy: 1.0000 - val_loss: 0.6013 - val_accuracy: 0.8528\n",
      "Epoch 28/30\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 5.4513e-05 - accuracy: 1.0000 - val_loss: 0.6108 - val_accuracy: 0.8532\n",
      "Epoch 29/30\n",
      "250/250 [==============================] - 2s 10ms/step - loss: 4.6682e-05 - accuracy: 1.0000 - val_loss: 0.6195 - val_accuracy: 0.8524\n",
      "Epoch 30/30\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 3.9960e-05 - accuracy: 1.0000 - val_loss: 0.6266 - val_accuracy: 0.8512\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_tr, y_tr,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_ts, y_ts),\n",
    "    batch_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 20:23:08.880857: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1034060000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 20:23:12.791370: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1034060000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  0.8512\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_tr, y_tr, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_ts, y_ts, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5677     Honestly, buying this movie was a waste of mon...\n",
      "8687     When I first watched this film, I thought that...\n",
      "1221     as a retired USAF MSG (aircraft maint. spec), ...\n",
      "8034     I was very happy and at the same time quite su...\n",
      "20403    This movie is told through the eyes of a young...\n",
      "                               ...                        \n",
      "20659    Most book adaptations are bad but this film le...\n",
      "5543     I really felt the movie was ahead of its time....\n",
      "23070    Movie industry is tricky business - because de...\n",
      "1304     The tagline for this show is, \"He's speaking h...\n",
      "8938     Kairo, or Pulse as it's known amongst English ...\n",
      "Name: text, Length: 2500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sentences_train, sentences_test, y_tr, y_ts= train_test_split(df['text'], df['label'], test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "print(sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "X_tr = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_ts = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1324 1779   10   15   13    2  398    4  247   40   28    4    1   87\n",
      "  385   99   11   25  107    8   56  109    1   62  171   11  372    6\n",
      "    1  272   18   40   35  602   12   21   55    1  777  272   65  612\n",
      "   10   15    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_tr = pad_sequences(X_tr, padding='post', maxlen=maxlen)\n",
    "X_ts = pad_sequences(X_ts, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(X_tr[0, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 50)           1554450   \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 50)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                510       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,554,971\n",
      "Trainable params: 1,554,971\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "embedding_dim = 50\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=maxlen))\n",
    "model_cnn.add(layers.GlobalMaxPool1D())\n",
    "model_cnn.add(layers.Dense(10, activation='relu'))\n",
    "model_cnn.add(layers.Dense(1, activation='sigmoid'))\n",
    "model_cnn.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "250/250 [==============================] - 6s 21ms/step - loss: 0.6881 - accuracy: 0.5636 - val_loss: 0.6766 - val_accuracy: 0.6512\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.6058 - accuracy: 0.7624 - val_loss: 0.5381 - val_accuracy: 0.7880\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 5s 21ms/step - loss: 0.3849 - accuracy: 0.8744 - val_loss: 0.4317 - val_accuracy: 0.8060\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.2308 - accuracy: 0.9316 - val_loss: 0.4117 - val_accuracy: 0.8080\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.1330 - accuracy: 0.9632 - val_loss: 0.4206 - val_accuracy: 0.8104\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 5s 21ms/step - loss: 0.0701 - accuracy: 0.9860 - val_loss: 0.4445 - val_accuracy: 0.8080\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.0356 - accuracy: 0.9972 - val_loss: 0.4651 - val_accuracy: 0.8068\n",
      "Epoch 8/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.0183 - accuracy: 0.9996 - val_loss: 0.4869 - val_accuracy: 0.8088\n",
      "Epoch 9/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.5044 - val_accuracy: 0.8084\n",
      "Epoch 10/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.5235 - val_accuracy: 0.8060\n",
      "Epoch 11/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.5407 - val_accuracy: 0.8036\n",
      "Epoch 12/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.5537 - val_accuracy: 0.8028\n",
      "Epoch 13/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.5675 - val_accuracy: 0.8012\n",
      "Epoch 14/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.5802 - val_accuracy: 0.8012\n",
      "Epoch 15/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5922 - val_accuracy: 0.8012\n",
      "Epoch 16/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.6039 - val_accuracy: 0.8024\n",
      "Epoch 17/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 8.1328e-04 - accuracy: 1.0000 - val_loss: 0.6161 - val_accuracy: 0.8012\n",
      "Epoch 18/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 6.5169e-04 - accuracy: 1.0000 - val_loss: 0.6274 - val_accuracy: 0.8032\n",
      "Epoch 19/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 5.2573e-04 - accuracy: 1.0000 - val_loss: 0.6388 - val_accuracy: 0.8032\n",
      "Epoch 20/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 4.2922e-04 - accuracy: 1.0000 - val_loss: 0.6504 - val_accuracy: 0.8040\n",
      "Epoch 21/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 3.5302e-04 - accuracy: 1.0000 - val_loss: 0.6603 - val_accuracy: 0.8032\n",
      "Epoch 22/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 2.9114e-04 - accuracy: 1.0000 - val_loss: 0.6690 - val_accuracy: 0.8028\n",
      "Epoch 23/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 2.4236e-04 - accuracy: 1.0000 - val_loss: 0.6794 - val_accuracy: 0.8016\n",
      "Epoch 24/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 2.0174e-04 - accuracy: 1.0000 - val_loss: 0.6897 - val_accuracy: 0.8020\n",
      "Epoch 25/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 1.6915e-04 - accuracy: 1.0000 - val_loss: 0.7004 - val_accuracy: 0.8020\n",
      "Epoch 26/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 1.4210e-04 - accuracy: 1.0000 - val_loss: 0.7107 - val_accuracy: 0.8024\n",
      "Epoch 27/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 1.2001e-04 - accuracy: 1.0000 - val_loss: 0.7206 - val_accuracy: 0.8028\n",
      "Epoch 28/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 1.0162e-04 - accuracy: 1.0000 - val_loss: 0.7300 - val_accuracy: 0.8032\n",
      "Epoch 29/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 8.6216e-05 - accuracy: 1.0000 - val_loss: 0.7401 - val_accuracy: 0.8028\n",
      "Epoch 30/30\n",
      "250/250 [==============================] - 5s 20ms/step - loss: 7.3319e-05 - accuracy: 1.0000 - val_loss: 0.7491 - val_accuracy: 0.8044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f38928ecfa0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn.fit(X_tr, y_tr, validation_data=(X_ts, y_ts), epochs=EPOCHS, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy:  0.8044\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_cnn.evaluate(X_tr, y_tr, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_cnn.evaluate(X_ts, y_ts, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow import keras\n",
    "from typing import *\n",
    "# from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior and posterior likelihoods modeled by a zero-mean, unit-variance Gaussian normal distribution and multivariate Gaussian distribution with learnable mean and variances, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero mean, unit variance multivariate normal\n",
    "def prior(kernel_size, bias_size, dtype=None):\n",
    "        n = kernel_size + bias_size\n",
    "        prior_model = keras.Sequential(\n",
    "            [\n",
    "                tfp.layers.DistributionLambda(\n",
    "                    lambda t: tfp.distributions.MultivariateNormalDiag(\n",
    "                        loc=tf.zeros(n), scale_diag=tf.ones(n)\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        return prior_model\n",
    "\n",
    "\n",
    "# variational multivariate normal (learnable means and variances)\n",
    "def posterior(kernel_size, bias_size, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    posterior_model = keras.Sequential(\n",
    "        [\n",
    "            tfp.layers.VariableLayer(\n",
    "                tfp.layers.MultivariateNormalTriL.params_size(n),\n",
    "                dtype=dtype\n",
    "            ),\n",
    "            tfp.layers.MultivariateNormalTriL(n)\n",
    "        ]\n",
    "    )\n",
    "    return posterior_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and compile model. The code below creates a hybrid BNN with 2 8-unit deterministic layers and a 2-unit probabilistic layer. The output is a normal distribution based on the parameters learned in the posterior functions/distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/avandebrook/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_probability/python/distributions/distribution.py:342: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 20:25:52.896910: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(\n",
    "    shape=(maxlen,), dtype=tf.float64\n",
    ")\n",
    "\n",
    "features = keras.layers.BatchNormalization()(inputs)\n",
    "features = keras.layers.Dense(8, activation='sigmoid')(features)\n",
    "features = keras.layers.Dense(8, activation='sigmoid')(features)\n",
    "\n",
    "distribution_params = tfp.layers.DenseVariational(\n",
    "    units=2,\n",
    "    make_prior_fn=prior,\n",
    "    make_posterior_fn=posterior,\n",
    "    kl_weight=1/TRAIN_SIZE\n",
    ")(features)\n",
    "\n",
    "outputs = tfp.layers.IndependentNormal(1)(distribution_params)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.CategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate the model on the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "79/79 [==============================] - 2s 9ms/step - loss: 7.7032e-04 - categorical_accuracy: 1.0000 - val_loss: 6.4128e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 2/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 6.6049e-04 - categorical_accuracy: 1.0000 - val_loss: 4.5872e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 3/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 4.5444e-04 - categorical_accuracy: 1.0000 - val_loss: 4.1834e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 4/30\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 4.5169e-04 - categorical_accuracy: 1.0000 - val_loss: 3.7436e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 5/30\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 3.4373e-04 - categorical_accuracy: 1.0000 - val_loss: 1.9970e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 6/30\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 3.2304e-04 - categorical_accuracy: 1.0000 - val_loss: 2.1839e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 2.3551e-04 - categorical_accuracy: 1.0000 - val_loss: 1.8969e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 2.0547e-04 - categorical_accuracy: 1.0000 - val_loss: 1.6516e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 2.2863e-04 - categorical_accuracy: 1.0000 - val_loss: 9.0770e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 1.1285e-04 - categorical_accuracy: 1.0000 - val_loss: 9.9006e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 8.0374e-05 - categorical_accuracy: 1.0000 - val_loss: 1.2009e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 5.3311e-05 - categorical_accuracy: 1.0000 - val_loss: 7.7444e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 1.4866e-05 - categorical_accuracy: 1.0000 - val_loss: 4.3475e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 4.1496e-05 - categorical_accuracy: 1.0000 - val_loss: 2.0308e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 7.8632e-05 - categorical_accuracy: 1.0000 - val_loss: 3.8722e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 4.1827e-05 - categorical_accuracy: 1.0000 - val_loss: 3.3169e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 5.0643e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7986e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3.6978e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7697e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 2.9156e-05 - categorical_accuracy: 1.0000 - val_loss: 1.9992e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 1.9372e-05 - categorical_accuracy: 1.0000 - val_loss: -1.7121e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 2.9409e-06 - categorical_accuracy: 1.0000 - val_loss: 2.7436e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3.5447e-05 - categorical_accuracy: 1.0000 - val_loss: -7.1810e-06 - val_categorical_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 3.1055e-05 - categorical_accuracy: 1.0000 - val_loss: 1.7655e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: -1.0002e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7688e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9.8547e-06 - categorical_accuracy: 1.0000 - val_loss: 1.4092e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 2.0045e-05 - categorical_accuracy: 1.0000 - val_loss: 2.1044e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 5.1576e-06 - categorical_accuracy: 1.0000 - val_loss: 1.8054e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 2.8344e-05 - categorical_accuracy: 1.0000 - val_loss: 4.7214e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 9.4441e-06 - categorical_accuracy: 1.0000 - val_loss: 3.4817e-05 - val_categorical_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 1.0533e-05 - categorical_accuracy: 1.0000 - val_loss: 5.8599e-06 - val_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f388e667c10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_tr,\n",
    "    y_tr,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    validation_data=tuple([X_ts, y_ts])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 2ms/step - loss: 3.9333e-05 - categorical_accuracy: 1.0000\n",
      "Training accuracy: 1.0\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 1.5225e-05 - categorical_accuracy: 1.0000\n",
      "Testing accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "clear_session()\n",
    "\n",
    "loss, accuracy = model.evaluate(X_tr, y_tr)\n",
    "print(f'Training accuracy: {accuracy}')\n",
    "\n",
    "loss, accuracy = model.evaluate(X_ts, y_ts)\n",
    "print(f'Testing accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.127760</td>\n",
       "      <td>2.224237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.011206</td>\n",
       "      <td>2.037277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.451542</td>\n",
       "      <td>2.138670</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.174447</td>\n",
       "      <td>2.259192</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.321685</td>\n",
       "      <td>2.312040</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Mean  Standard Deviation  Label\n",
       "0 -0.127760            2.224237      1\n",
       "1 -0.011206            2.037277      0\n",
       "2  0.451542            2.138670      0\n",
       "3  0.174447            2.259192      1\n",
       "4 -0.321685            2.312040      0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREDICTION_ITERATIONS = 100\n",
    "\n",
    "results = {\n",
    "    'Mean': [],\n",
    "    'Standard Deviation': [],\n",
    "    'Label': [],\n",
    "}\n",
    "\n",
    "# predict on test samples\n",
    "for i, sample in enumerate(X_ts[0:50, :]):\n",
    "    sample = sample.reshape(1, sample.shape[0])\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(PREDICTION_ITERATIONS):\n",
    "        predictions.append(model.predict(sample))\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=1)\n",
    "\n",
    "    results['Mean'].append(np.average(predictions, axis=1)[0])\n",
    "    results['Standard Deviation'].append(np.std(predictions, axis=1)[0])\n",
    "    results['Label'].append(y_ts.array[i])\n",
    "\n",
    "results = pd.DataFrame(data=results)\n",
    "results.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0250550d6d548e4e5a6cdc83ec66018a6064bd14f2654c3f43a648b49f3a281d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
